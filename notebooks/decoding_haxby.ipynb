{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5446c2b3",
   "metadata": {},
   "source": [
    "### Decoding from fMRI data\n",
    "\n",
    "In this notebook we will use the Haxby et al. data that were prepared in the Data Setup notebook to perform classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f29378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut, KFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import f_classif, SelectKBest, VarianceThreshold\n",
    "import warnings\n",
    "from utils import get_subject_runs\n",
    "import nilearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nilearn.maskers import NiftiMasker\n",
    "import nilearn.plotting\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from nilearn.decoding.searchlight import search_light\n",
    "from sklearn import neighbors\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "base_dir = '/Users/poldrack/data_unsynced/ds000105'\n",
    "h5_file = os.path.join(base_dir, 'derivatives/cleaned/haxby_data_cleaned.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d1320",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a5a41d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 1404)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['bottle', 'cat', 'chair', 'face', 'house', 'scissors',\n",
       "       'scrambledpix', 'shoe'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_subject_data(\n",
    "    subject,\n",
    "    h5_file,\n",
    "    bids_dir,\n",
    "    data_key='vtmaskdata',\n",
    "    condmeans=False,\n",
    "    condition_subset=None,\n",
    "):\n",
    "    assert data_key in ['braindata', 'vtmaskdata', 'difumodata']\n",
    "    meanstr = 'mean_' if condmeans else ''\n",
    "    runs = get_subject_runs(subject, bids_dir)\n",
    "    X, metadata_df = None, None\n",
    "\n",
    "    with h5py.File(h5_file, 'r') as hf:\n",
    "        for run in runs:\n",
    "            if X is None:\n",
    "                X = hf[f'sub-{subject}/run-{run}/{meanstr + data_key}'][:]\n",
    "            else:\n",
    "                X = np.vstack(\n",
    "                    (X, hf[f'sub-{subject}/run-{run}/{meanstr + data_key}'][:])\n",
    "                )\n",
    "            if metadata_df is None:\n",
    "                conditions = [\n",
    "                    i.decode('utf-8')\n",
    "                    for i in hf[\n",
    "                        f'sub-{subject}/run-{run}/{meanstr}conditions'\n",
    "                    ][:]\n",
    "                ]\n",
    "                metadata_df = pd.DataFrame(\n",
    "                    {'conditions': conditions, 'run': run}\n",
    "                )\n",
    "            else:\n",
    "                conditions = [\n",
    "                    i.decode('utf-8')\n",
    "                    for i in hf[\n",
    "                        f'sub-{subject}/run-{run}/{meanstr}conditions'\n",
    "                    ][:]\n",
    "                ]\n",
    "                metadata_df = pd.concat(\n",
    "                    [\n",
    "                        metadata_df,\n",
    "                        pd.DataFrame({'conditions': conditions, 'run': run}),\n",
    "                    ]\n",
    "                )\n",
    "    if condition_subset is not None:\n",
    "        metadata_df = metadata_df[\n",
    "            metadata_df['conditions'].isin(condition_subset)\n",
    "        ]\n",
    "        X = X[metadata_df.index]\n",
    "    assert X.shape[0] == metadata_df.shape[0]\n",
    "    return X, metadata_df\n",
    "\n",
    "\n",
    "X, metadata_df = get_subject_data(\n",
    "    1, h5_file, base_dir, condmeans=True, data_key='vtmaskdata'\n",
    ")\n",
    "print(X.shape)\n",
    "metadata_df.conditions.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf3da73",
   "metadata": {},
   "source": [
    "### Run decoding model\n",
    "\n",
    "Use a leave-one-run-out crossvalidation scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cdc445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross-validation with SGDClassifier()\n",
      "Subject 1\n",
      "Mean accuracy (orig): 0.802\n",
      "\n",
      "Subject 2\n",
      "Mean accuracy (orig): 0.646\n",
      "\n",
      "Subject 3\n",
      "Mean accuracy (orig): 0.729\n",
      "\n",
      "Subject 4\n",
      "Mean accuracy (orig): 0.635\n",
      "\n",
      "Subject 5\n",
      "Mean accuracy (orig): 0.875\n",
      "\n",
      "Subject 6\n",
      "Mean accuracy (orig): 0.729\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SGDL2_vtmask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.802083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.729167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.635417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.729167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SGDL2_vtmask\n",
       "0      0.802083\n",
       "1      0.645833\n",
       "2      0.729167\n",
       "3      0.635417\n",
       "4      0.875000\n",
       "5      0.729167"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# leave one run out cross validation\n",
    "\n",
    "\n",
    "def run_cv_subject(\n",
    "    data,\n",
    "    metadata_df,\n",
    "    nfeatures=None,\n",
    "    shuffle=False,\n",
    "    varthresh=None,\n",
    "    clf=None,\n",
    "    suppress_warnings=False,\n",
    "    standardize=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform cross-validation for decoding analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df (pandas.DataFrame): The input data frame containing the features and labels.\n",
    "    - nfeatures (int): The number of features to select using ANOVA. Default is 1000, or None to disable.\n",
    "    - shuffle_y (bool): Whether to shuffle the labels. Default is False\n",
    "    - varthresh (float): The variance threshold for feature selection. Default is 0, or None to disable.\n",
    "    - clf (sklearn classifier): The classifier to use. Default is None, which uses a Support Vector Machine\n",
    "\n",
    "    Returns:\n",
    "    - accs (list): A list of accuracy scores for each cross-validation fold.\n",
    "    \"\"\"\n",
    "    if clf is None:\n",
    "        clf = SGDClassifier()   # by default is like SVM with l2 regularization\n",
    "\n",
    "    # Suppress warnings from scikit-learn\n",
    "    if suppress_warnings:\n",
    "        warnings.filterwarnings('ignore', category=UserWarning)\n",
    "        warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "    # Leave one run out cross-validation\n",
    "    logo = LeaveOneGroupOut()\n",
    "    assert data.shape[0] == metadata_df.shape[0]\n",
    "\n",
    "    if shuffle:\n",
    "        metadata_df.conditions = np.random.permutation(metadata_df.conditions)\n",
    "\n",
    "    # perform variance thresholding with default of zero\n",
    "    if varthresh is not None:\n",
    "        vt = VarianceThreshold(threshold=varthresh)\n",
    "        data = vt.fit_transform(data)\n",
    "\n",
    "    accs = []\n",
    "    coefs = None\n",
    "    for train_index, test_index in logo.split(data, groups=metadata_df.run):\n",
    "        train_X = data[train_index]\n",
    "        assert train_X.shape[0] == len(train_index)\n",
    "        assert train_X.shape[1] == data.shape[1]\n",
    "        test_X = data[test_index]\n",
    "        train_y = metadata_df.conditions.iloc[train_index]\n",
    "        test_y = metadata_df.conditions.iloc[test_index]\n",
    "\n",
    "        if standardize:\n",
    "            scaler = StandardScaler()\n",
    "            train_X = scaler.fit_transform(train_X)\n",
    "            test_X = scaler.transform(test_X)\n",
    "\n",
    "        if nfeatures is not None:\n",
    "            # Feature selection based on training data only\n",
    "            selector = SelectKBest(score_func=f_classif, k=nfeatures)\n",
    "            train_data_selected = selector.fit_transform(train_X, train_y)\n",
    "            test_data_selected = selector.transform(test_X)\n",
    "        else:\n",
    "            train_data_selected = train_X\n",
    "            test_data_selected = test_X\n",
    "\n",
    "        clf.fit(train_data_selected, train_y)\n",
    "        if hasattr(clf, 'coef_'):\n",
    "            if coefs is None:\n",
    "                coefs = clf.coef_\n",
    "            else:\n",
    "\n",
    "                coefs += clf.coef_\n",
    "        acc = accuracy_score(test_y, clf.predict(test_data_selected))\n",
    "        accs.append(acc)\n",
    "    if coefs is not None:\n",
    "        mean_coefs = coefs / len(accs)\n",
    "    else:\n",
    "        mean_coefs = None\n",
    "    return accs, mean_coefs\n",
    "\n",
    "\n",
    "def get_runs_from_hf(hf, subject):\n",
    "    runs = [int(i.split('-')[-1]) for i in hf[subject].keys()]\n",
    "    runs.sort()\n",
    "    return runs\n",
    "\n",
    "\n",
    "def run_cv(\n",
    "    clf=None,\n",
    "    shuffle=False,\n",
    "    nruns=1,\n",
    "    nfeatures=None,\n",
    "    varthresh=None,\n",
    "    data_key='brain',\n",
    "    condmeans=False,\n",
    "    condition_subset=None,\n",
    "):\n",
    "    data_key = data_key + 'data'\n",
    "    if clf is None:\n",
    "        clf = SGDClassifier()\n",
    "    print(f'Running cross-validation with {clf}')\n",
    "    if nfeatures is not None:\n",
    "        print(f'Selecting best {nfeatures} features: ')\n",
    "    if varthresh is not None:\n",
    "        print(f'Performing variance thresholding with threshold {varthresh}')\n",
    "    if shuffle:\n",
    "        print('Using shuffled labels')\n",
    "    accs = {}\n",
    "    coefs = {}\n",
    "\n",
    "    for subject in range(1, 7):\n",
    "        subdata, metadata_df = get_subject_data(\n",
    "            subject,\n",
    "            h5_file,\n",
    "            base_dir,\n",
    "            condmeans=condmeans,\n",
    "            data_key=data_key,\n",
    "            condition_subset=condition_subset,\n",
    "        )\n",
    "        print(f'Subject {subject}')\n",
    "        shuffled_string = 'shuffled' if shuffle else 'orig'\n",
    "\n",
    "        accs[subject] = {shuffled_string: []}\n",
    "\n",
    "        for i in range(nruns):\n",
    "            acc_, coef_ = run_cv_subject(\n",
    "                subdata,\n",
    "                metadata_df,\n",
    "                nfeatures=nfeatures,\n",
    "                shuffle=shuffle,\n",
    "                varthresh=varthresh,\n",
    "                clf=clf,\n",
    "            )\n",
    "            if shuffle is False:\n",
    "                coefs[subject] = coef_\n",
    "            accs[subject][shuffled_string].append(np.mean(acc_))\n",
    "        print(\n",
    "            f'Mean accuracy ({shuffled_string}): {np.mean(accs[subject][shuffled_string]):.03}'\n",
    "        )\n",
    "        print('')\n",
    "    return accs, coefs\n",
    "\n",
    "\n",
    "# run crossvalidation across subjects with default settings\n",
    "# need to run without thresholding or feature selection to get the full set of coefs\n",
    "data_key = 'vtmask'\n",
    "condmeans = True\n",
    "condition_subset = None   # ['face', 'house', 'cat']\n",
    "\n",
    "accs, coefs = run_cv(\n",
    "    shuffle=False,\n",
    "    #nshuffles=10,\n",
    "    data_key=data_key,\n",
    "    condmeans=condmeans,\n",
    "    varthresh=None,\n",
    "    nfeatures=None,\n",
    "    condition_subset=condition_subset,\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame({f'SGDL2_{data_key}': [accs[i]['orig'][0] for i in accs.keys()]})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb3b36",
   "metadata": {},
   "source": [
    "Shuffle to assess vs chance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca66ca1f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross-validation with SGDClassifier()\n",
      "Using shuffled labels\n",
      "Subject 1\n",
      "Mean accuracy (shuffled): 0.112\n",
      "\n",
      "Subject 2\n",
      "Mean accuracy (shuffled): 0.112\n",
      "\n",
      "Subject 3\n",
      "Mean accuracy (shuffled): 0.112\n",
      "\n",
      "Subject 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accs_shuf, _ = run_cv(\n",
    "    shuffle=True,\n",
    "    nruns=1000,\n",
    "    data_key=data_key,\n",
    "    condmeans=condmeans,\n",
    "    varthresh=None,\n",
    "    nfeatures=None,\n",
    "    condition_subset=condition_subset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(accs_shuf[1]['shuffled']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4de724",
   "metadata": {},
   "source": [
    "#### Visualizing the coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab7236",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_subject_vt_mask(subject, vtmask_dir, res=3):\n",
    "    vt_mask_file = os.path.join(\n",
    "        vtmask_dir,\n",
    "        f'sub-{subject}_mask4vt_space-MNI152NLin2009cAsym_res-{res}.nii.gz',\n",
    "    )\n",
    "    return nib.load(vt_mask_file)\n",
    "\n",
    "\n",
    "def visualize_coefs(coefs, conditions, vtmask_dir):\n",
    "\n",
    "    # plot the mean coefficients\n",
    "    for subject in range(1, 2):\n",
    "        mask_img = get_subject_vt_mask(subject, vtmask_dir)\n",
    "        masker = NiftiMasker(mask_img)\n",
    "        masker.fit()\n",
    "        print(coefs[subject].shape)\n",
    "        coef_img = masker.inverse_transform(coefs[subject])\n",
    "        for c in range(coefs[subject].shape[0]):\n",
    "            nilearn.plotting.plot_stat_map(\n",
    "                coef_img.slicer[..., c],\n",
    "                display_mode='z',\n",
    "                cut_coords=[-21, -18, -15, -9, 0],\n",
    "                title=f'Subject {subject}, condition {conditions[c]}',\n",
    "            )\n",
    "\n",
    "\n",
    "if condition_subset is None:\n",
    "    conditions = metadata_df.conditions.unique().tolist()\n",
    "else:\n",
    "    conditions = condition_subset\n",
    "vtmask_dir = os.path.join(base_dir, 'derivatives/vtmasks')\n",
    "visualize_coefs(coefs, conditions, vtmask_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d436a0",
   "metadata": {},
   "source": [
    "### Effect of different regularization schemes\n",
    "\n",
    "In the previous model, we used a support vector machine with an L2 penalty, which penalizes the sum of squared weights. We can also look at the effect using a different penalty, namely an L1 penalty, which penalizes based on the sum of absolute weights, and leads in general to sparser solutions (more zero weights).\n",
    "\n",
    "We can set the `LinearSVC` classifier to use L1 penalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ff7b4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "accs_L1, coefs_L1 = run_cv(\n",
    "    shuffle=False,\n",
    "    data_key=data_key,\n",
    "    condmeans=condmeans,\n",
    "    clf=SGDClassifier(penalty='l1', alpha=0.01),\n",
    "    varthresh=None,\n",
    "    nfeatures=None,\n",
    ")\n",
    "results_df[f'SGDL1_{data_key}'] = [accs_L1[i]['orig'] for i in accs_L1.keys()]\n",
    "visualize_coefs(coefs_L1, conditions, vtmask_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a58b2",
   "metadata": {},
   "source": [
    "#### Using dimensionality-reduced data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2578a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'difumo'\n",
    "accs_difumo, _ = run_cv(\n",
    "    shuffle=False,\n",
    "    data_key=data_key,\n",
    "    condmeans=condmeans,\n",
    "    varthresh=0,\n",
    "    nfeatures=50,\n",
    ")\n",
    "results_df[f'SGDL2_{data_key}'] = [accs_difumo[i]['orig'] for i in accs_difumo.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1651b602",
   "metadata": {},
   "source": [
    "### Using whole-brain data with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b78575",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'brain'\n",
    "accs_wholebrain, _ = run_cv(\n",
    "    shuffle=False,\n",
    "    data_key=data_key,\n",
    "    condmeans=condmeans,\n",
    "    varthresh=0,\n",
    "    nfeatures=500,\n",
    ")\n",
    "results_df[f'SGDL2_{data_key}'] = [accs_wholebrain[i]['orig'] for i in accs_wholebrain.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21ffc6",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "\n",
    "Now we will use a nonlinear model: a multi-layer perceptron, which has a single layer of modifiable weights.  We use a single hidden layer with 100 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd90f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key = 'vtmask'\n",
    "\n",
    "accs_MLP, coefs_MLP = run_cv(\n",
    "    shuffle=False,\n",
    "    data_key=data_key,\n",
    "    condmeans=condmeans,\n",
    "    clf=MLPClassifier(hidden_layer_sizes=(128), max_iter=500),\n",
    "    varthresh=None,\n",
    "    nfeatures=None,\n",
    ")\n",
    "results_df[f'MLP_{data_key}'] = [accs_MLP[i]['orig'] for i in accs_MLP.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f17b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebaeddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.boxplot(data=results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e58f2",
   "metadata": {},
   "source": [
    "### Searchlight decoding using surface data\n",
    "\n",
    "Based on [nilearn tutorial](https://nilearn.github.io/stable/auto_examples/02_decoding/plot_haxby_searchlight_surface.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b70a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_surface_data(\n",
    "    subject,\n",
    "    h5_file,\n",
    "    bids_dir,\n",
    "    condmeans=True,\n",
    "    hemispheres=None,\n",
    "    condition_subset=None,\n",
    "):\n",
    "    if hemispheres is None:\n",
    "        hemispheres = ['L', 'R']\n",
    "    # replace left/right with L/R\n",
    "    hemispheres = [h if h in ['L', 'R'] else h[0].upper() for h in hemispheres]\n",
    "    meanstr = 'mean_' if condmeans else ''\n",
    "    runs = get_subject_runs(subject, bids_dir)\n",
    "    X, metadata_df = None, None\n",
    "\n",
    "    with h5py.File(h5_file, 'r') as hf:\n",
    "\n",
    "        for run in runs:\n",
    "            data = [hf[f'sub-{subject}/run-{run}/{meanstr + h}'][:] for h in hemispheres]\n",
    "            combined_data = np.hstack((\n",
    "                  data\n",
    "                ))\n",
    "            if X is None:\n",
    "                X = combined_data\n",
    "            else:\n",
    "                X = np.vstack(\n",
    "                    (X, combined_data)\n",
    "                )\n",
    "            if metadata_df is None:\n",
    "                conditions = [\n",
    "                    i.decode('utf-8')\n",
    "                    for i in hf[\n",
    "                        f'sub-{subject}/run-{run}/{meanstr}conditions'\n",
    "                    ][:]\n",
    "                ]\n",
    "                metadata_df = pd.DataFrame(\n",
    "                    {'conditions': conditions, 'run': run}\n",
    "                )\n",
    "            else:\n",
    "                conditions = [\n",
    "                    i.decode('utf-8')\n",
    "                    for i in hf[\n",
    "                        f'sub-{subject}/run-{run}/{meanstr}conditions'\n",
    "                    ][:]\n",
    "                ]\n",
    "                metadata_df = pd.concat(\n",
    "                    [\n",
    "                        metadata_df,\n",
    "                        pd.DataFrame({'conditions': conditions, 'run': run}),\n",
    "                    ]\n",
    "                )\n",
    "    if condition_subset is not None:\n",
    "        metadata_df = metadata_df[\n",
    "            metadata_df['conditions'].isin(condition_subset)\n",
    "        ]\n",
    "        X = X[metadata_df.index]\n",
    "    assert X.shape[0] == metadata_df.shape[0]\n",
    "    return X, metadata_df\n",
    "\n",
    "\n",
    "h5_surface_file = os.path.join(base_dir, 'derivatives/cleaned/haxby_surface_data_cleaned.h5')\n",
    "X, metadata_df = get_subject_surface_data(\n",
    "    1, h5_surface_file, base_dir, condmeans=True, \n",
    "    hemispheres=['L'],\n",
    "   # condition_subset=[\"face\", \"house\"]\n",
    ")\n",
    "print(X.shape)\n",
    "metadata_df.conditions.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def searchlight_adjacency(hemi, radius=3, surf='infl'):\n",
    "    assert hemi in ['left', 'right']\n",
    "    fsaverage = nilearn.datasets.fetch_surf_fsaverage(mesh=\"fsaverage5\")\n",
    "    infl_mesh = fsaverage[f\"{surf}_{hemi}\"]\n",
    "    coords, _ = nilearn.surface.load_surf_mesh(infl_mesh)\n",
    "    nn = neighbors.NearestNeighbors(radius=radius)\n",
    "    return nn.fit(coords).radius_neighbors_graph(coords).tolil()\n",
    "\n",
    "def run_searchlight(subject, h5_surface_file, base_dir,\n",
    "                    radius=3,\n",
    "                    condition_subset=None,\n",
    "                    condmeans=True):\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for hemis in ['left', 'right']:\n",
    "    \n",
    "        adjacency = searchlight_adjacency(hemis, radius)\n",
    "\n",
    "        X, metadata_df = get_subject_surface_data(\n",
    "            subject, h5_surface_file, base_dir, \n",
    "            condmeans=condmeans, \n",
    "            hemispheres=[hemis],\n",
    "            condition_subset=condition_subset\n",
    "        )\n",
    "\n",
    "        # Simple linear estimator preceded by a normalization step\n",
    "        estimator = make_pipeline(StandardScaler(), SGDClassifier())\n",
    "\n",
    "        # Define cross-validation scheme\n",
    "        # we can use this to do leave-one-run-out with shuffle turned off since there are 12 runs\n",
    "        # in order in the data, so each fold leaves out two runs\n",
    "        cv = KFold(n_splits=6, shuffle=False)\n",
    "\n",
    "        # Cross-validated search light\n",
    "        scores[hemis] = search_light(X, metadata_df.conditions.values, estimator, adjacency, cv=cv, n_jobs=12)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2205ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fsaverage = nilearn.datasets.fetch_surf_fsaverage(mesh=\"fsaverage5\")\n",
    "\n",
    "for subject in range(1, 7):\n",
    "    scores = run_searchlight(subject, h5_surface_file, base_dir, condition_subset=condition_subset)\n",
    "\n",
    "    fig, axes = plt.subplots(subplot_kw={'projection': '3d'}, nrows=2, ncols=2)\n",
    "\n",
    "    axes = axes.flatten()\n",
    "    surf = 'infl'\n",
    "    ctr = 0\n",
    "\n",
    "    threshold = 0.2\n",
    "    for hemis in ['left', 'right']:\n",
    "        for i, view in enumerate(['lateral', 'medial',]):      \n",
    "            infl_mesh = fsaverage[f\"{surf}_{hemis}\"]                           \n",
    "            nilearn.plotting.plot_surf_stat_map(\n",
    "                infl_mesh,\n",
    "                scores[hemis],\n",
    "                view=view,\n",
    "                colorbar=True,\n",
    "                threshold=threshold,\n",
    "                bg_map=fsaverage[f\"sulc_{hemis}\"],\n",
    "                title=f\"Sub {subject}, {hemis} hemis\",\n",
    "                axes=axes[ctr],\n",
    "            )\n",
    "            ctr += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b35b479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
