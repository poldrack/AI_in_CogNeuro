{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "103889d1",
   "metadata": {},
   "source": [
    "### Decoding from fMRI data\n",
    "\n",
    "In this notebook we will use the Haxby et al. data that were prepared in the Data Setup notebook to perform classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3729c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import f_classif, SelectKBest, VarianceThreshold\n",
    "import warnings\n",
    "from utils import get_subject_runs\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nilearn.maskers import NiftiMasker\n",
    "import nilearn.plotting\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "base_dir = '/Users/poldrack/data_unsynced/ds000105'\n",
    "h5_file = os.path.join(base_dir, 'derivatives/cleaned/haxby_data_cleaned.h5')\n",
    "\n",
    "device = (\n",
    "    'cuda'\n",
    "    if torch.cuda.is_available()\n",
    "    else 'mps'\n",
    "    if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "print(f'Using device {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e3294",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adefa84d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_subject_data(\n",
    "    subject,\n",
    "    h5_file,\n",
    "    bids_dir,\n",
    "    data_key='vtmaskdata',\n",
    "    condmeans=False,\n",
    "    condition_subset=None,\n",
    "):\n",
    "    assert data_key in ['braindata', 'vtmaskdata', 'difumodata']\n",
    "    meanstr = 'mean_' if condmeans else ''\n",
    "    runs = get_subject_runs(subject, bids_dir)\n",
    "    X, metadata_df = None, None\n",
    "\n",
    "    with h5py.File(h5_file, 'r') as hf:\n",
    "        for run in runs:\n",
    "            if X is None:\n",
    "                X = hf[f'sub-{subject}/run-{run}/{meanstr + data_key}'][:]\n",
    "            else:\n",
    "                X = np.vstack(\n",
    "                    (X, hf[f'sub-{subject}/run-{run}/{meanstr + data_key}'][:])\n",
    "                )\n",
    "            if metadata_df is None:\n",
    "                conditions = [\n",
    "                    i.decode('utf-8')\n",
    "                    for i in hf[\n",
    "                        f'sub-{subject}/run-{run}/{meanstr}conditions'\n",
    "                    ][:]\n",
    "                ]\n",
    "                metadata_df = pd.DataFrame(\n",
    "                    {'conditions': conditions, 'run': run}\n",
    "                )\n",
    "            else:\n",
    "                conditions = [\n",
    "                    i.decode('utf-8')\n",
    "                    for i in hf[\n",
    "                        f'sub-{subject}/run-{run}/{meanstr}conditions'\n",
    "                    ][:]\n",
    "                ]\n",
    "                metadata_df = pd.concat(\n",
    "                    [\n",
    "                        metadata_df,\n",
    "                        pd.DataFrame({'conditions': conditions, 'run': run}),\n",
    "                    ]\n",
    "                )\n",
    "    if condition_subset is not None:\n",
    "        metadata_df = metadata_df[\n",
    "            metadata_df['conditions'].isin(condition_subset)\n",
    "        ]\n",
    "        X = X[metadata_df.index]\n",
    "    assert X.shape[0] == metadata_df.shape[0]\n",
    "    return X, metadata_df\n",
    "\n",
    "\n",
    "X, metadata_df = get_subject_data(\n",
    "    1, h5_file, base_dir, condmeans=True, data_key='vtmaskdata'\n",
    ")\n",
    "print(X.shape)\n",
    "metadata_df.conditions.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f3563d",
   "metadata": {},
   "source": [
    "### Run decoding model\n",
    "\n",
    "Use a leave-one-run-out crossvalidation scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# leave one run out cross validation\n",
    "\n",
    "\n",
    "def run_cv_subject(\n",
    "    data,\n",
    "    metadata_df,\n",
    "    nfeatures=None,\n",
    "    shuffle=False,\n",
    "    varthresh=None,\n",
    "    clf=None,\n",
    "    suppress_warnings=False,\n",
    "    standardize=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Perform cross-validation for decoding analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - data_df (pandas.DataFrame): The input data frame containing the features and labels.\n",
    "    - nfeatures (int): The number of features to select using ANOVA. Default is 1000, or None to disable.\n",
    "    - shuffle_y (bool): Whether to shuffle the labels. Default is False\n",
    "    - varthresh (float): The variance threshold for feature selection. Default is 0, or None to disable.\n",
    "    - clf (sklearn classifier): The classifier to use. Default is None, which uses a Support Vector Machine\n",
    "\n",
    "    Returns:\n",
    "    - accs (list): A list of accuracy scores for each cross-validation fold.\n",
    "    \"\"\"\n",
    "    if clf is None:\n",
    "        clf = SGDClassifier()   # by default is like SVM with l2 regularization\n",
    "\n",
    "    # Suppress warnings from scikit-learn\n",
    "    if suppress_warnings:\n",
    "        warnings.filterwarnings('ignore', category=UserWarning)\n",
    "        warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "    # Leave one run out cross-validation\n",
    "    logo = LeaveOneGroupOut()\n",
    "    assert data.shape[0] == metadata_df.shape[0]\n",
    "\n",
    "    if shuffle:\n",
    "        metadata_df.conditions = np.random.permutation(metadata_df.conditions)\n",
    "\n",
    "    # perform variance thresholding with default of zero\n",
    "    if varthresh is not None:\n",
    "        vt = VarianceThreshold(threshold=varthresh)\n",
    "        data = vt.fit_transform(data)\n",
    "\n",
    "    accs = []\n",
    "    coefs = None\n",
    "    for train_index, test_index in logo.split(data, groups=metadata_df.run):\n",
    "        train_X = data[train_index]\n",
    "        assert train_X.shape[0] == len(train_index)\n",
    "        assert train_X.shape[1] == data.shape[1]\n",
    "        test_X = data[test_index]\n",
    "        train_y = metadata_df.conditions.iloc[train_index]\n",
    "        test_y = metadata_df.conditions.iloc[test_index]\n",
    "\n",
    "        if standardize:\n",
    "            scaler = StandardScaler()\n",
    "            train_X = scaler.fit_transform(train_X)\n",
    "            test_X = scaler.transform(test_X)\n",
    "\n",
    "        if nfeatures is not None:\n",
    "            # Feature selection based on training data only\n",
    "            selector = SelectKBest(score_func=f_classif, k=nfeatures)\n",
    "            train_data_selected = selector.fit_transform(train_X, train_y)\n",
    "            test_data_selected = selector.transform(test_X)\n",
    "        else:\n",
    "            train_data_selected = train_X\n",
    "            test_data_selected = test_X\n",
    "\n",
    "        clf.fit(train_data_selected, train_y)\n",
    "        if hasattr(clf, 'coef_'):\n",
    "            if coefs is None:\n",
    "                coefs = clf.coef_\n",
    "            else:\n",
    "\n",
    "                coefs += clf.coef_\n",
    "        acc = accuracy_score(test_y, clf.predict(test_data_selected))\n",
    "        accs.append(acc)\n",
    "    if coefs is not None:\n",
    "        mean_coefs = coefs / len(accs)\n",
    "    else:\n",
    "        mean_coefs = None\n",
    "    return accs, mean_coefs\n",
    "\n",
    "\n",
    "def get_runs_from_hf(hf, subject):\n",
    "    runs = [int(i.split('-')[-1]) for i in hf[subject].keys()]\n",
    "    runs.sort()\n",
    "    return runs\n",
    "\n",
    "\n",
    "def run_cv(\n",
    "    clf=None,\n",
    "    shuffle=False,\n",
    "    nfeatures=None,\n",
    "    varthresh=None,\n",
    "    data_key='braindata',\n",
    "    condmeans=False,\n",
    "    condition_subset=None,\n",
    "):\n",
    "    if clf is None:\n",
    "        clf = SGDClassifier()\n",
    "    print(f'Running cross-validation with {clf}')\n",
    "    if nfeatures is not None:\n",
    "        print(f'Selecting best {nfeatures} features: ')\n",
    "    if varthresh is not None:\n",
    "        print(f'Performing variance thresholding with threshold {varthresh}')\n",
    "    if shuffle:\n",
    "        print('Including shuffled data')\n",
    "    accs = {}\n",
    "    coefs = {}\n",
    "\n",
    "    # would usually run this many more times for shuffled data\n",
    "    nshuffles = 1\n",
    "    shuffle_vals = [False, True] if shuffle else [False]\n",
    "\n",
    "    for subject in range(1, 7):\n",
    "        subdata, metadata_df = get_subject_data(\n",
    "            subject,\n",
    "            h5_file,\n",
    "            base_dir,\n",
    "            condmeans=condmeans,\n",
    "            data_key=data_key,\n",
    "            condition_subset=condition_subset,\n",
    "        )\n",
    "        print(f'Subject {subject}')\n",
    "        print(f'data shape: {subdata.shape}')\n",
    "        accs[subject] = {}\n",
    "        for shuffle_y in shuffle_vals:\n",
    "            shuffled_string = 'shuffled' if shuffle_y else 'orig'\n",
    "            nruns = nshuffles if shuffle_y else 1\n",
    "            accs[subject][shuffled_string] = 0\n",
    "            for i in range(nruns):\n",
    "                acc_, coef_ = run_cv_subject(\n",
    "                    subdata,\n",
    "                    metadata_df,\n",
    "                    nfeatures=nfeatures,\n",
    "                    shuffle=shuffle_y,\n",
    "                    varthresh=varthresh,\n",
    "                    clf=clf,\n",
    "                )\n",
    "                if shuffle_y is False:\n",
    "                    coefs[subject] = coef_\n",
    "                accs[subject][shuffled_string] += np.mean(acc_)\n",
    "            accs[subject][shuffled_string] /= nshuffles\n",
    "            print(\n",
    "                f'Mean accuracy ({shuffled_string}): {np.mean(accs[subject][shuffled_string]):.03}'\n",
    "            )\n",
    "\n",
    "        print('')\n",
    "    return accs, coefs\n",
    "\n",
    "\n",
    "# run crossvalidation across subjects with default settings\n",
    "# need to run without thresholding or feature selection to get the full set of coefs\n",
    "data_key = 'vtmaskdata'\n",
    "condmeans = True\n",
    "condition_subset = None   # ['face', 'house', 'cat']\n",
    "\n",
    "accs, coefs = run_cv(\n",
    "    shuffle=False,\n",
    "    data_key=data_key,\n",
    "    condmeans=condmeans,\n",
    "    varthresh=None,\n",
    "    nfeatures=None,\n",
    "    condition_subset=condition_subset,\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame({'SGD_l2': [accs[i]['orig'] for i in accs.keys()]})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3d09f",
   "metadata": {},
   "source": [
    "#### Visualizing the coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c143a8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_subject_vt_mask(subject, vtmask_dir, res=3):\n",
    "    vt_mask_file = os.path.join(\n",
    "        vtmask_dir,\n",
    "        f'sub-{subject}_mask4vt_space-MNI152NLin2009cAsym_res-{res}.nii.gz',\n",
    "    )\n",
    "    return nib.load(vt_mask_file)\n",
    "\n",
    "\n",
    "def visualize_coefs(coefs, conditions, vtmask_dir):\n",
    "\n",
    "    # plot the mean coefficients\n",
    "    for subject in range(1, 2):\n",
    "        mask_img = get_subject_vt_mask(subject, vtmask_dir)\n",
    "        masker = NiftiMasker(mask_img)\n",
    "        masker.fit()\n",
    "        print(coefs[subject].shape)\n",
    "        coef_img = masker.inverse_transform(coefs[subject])\n",
    "        for c in range(coefs[subject].shape[0]):\n",
    "            nilearn.plotting.plot_stat_map(\n",
    "                coef_img.slicer[..., c],\n",
    "                display_mode='z',\n",
    "                cut_coords=[-21, -18, -15, -9, 0],\n",
    "                title=f'Subject {subject}, condition {conditions[c]}',\n",
    "            )\n",
    "\n",
    "\n",
    "if condition_subset is None:\n",
    "    conditions = metadata_df.conditions.unique().tolist()\n",
    "else:\n",
    "    conditions = condition_subset\n",
    "vtmask_dir = os.path.join(base_dir, 'derivatives/vtmasks')\n",
    "visualize_coefs(coefs, conditions, vtmask_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8500f13",
   "metadata": {},
   "source": [
    "### Effect of different regularization schemes\n",
    "\n",
    "In the previous model, we used a support vector machine with an L2 penalty, which penalizes the sum of squared weights. We can also look at the effect using a different penalty, namely an L1 penalty, which penalizes based on the sum of absolute weights, and leads in general to sparser solutions (more zero weights).\n",
    "\n",
    "We can set the `LinearSVC` classifier to use L1 penalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82457c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "accs_L1, coefs_L1 = run_cv(\n",
    "    shuffle=False,\n",
    "    data_key=data_key,\n",
    "    condmeans=condmeans,\n",
    "    clf=SGDClassifier(penalty='l1', alpha=0.01),\n",
    "    varthresh=None,\n",
    "    nfeatures=None,\n",
    ")\n",
    "results_df['SGD_l1'] = [accs_L1[i]['orig'] for i in accs_L1.keys()]\n",
    "visualize_coefs(coefs_L1, conditions, vtmask_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1421b48",
   "metadata": {},
   "source": [
    "### Multi-layer perceptron\n",
    "\n",
    "Now we will use a nonlinear model: a multi-layer perceptron, which has a single layer of modifiable weights.  We use a single hidden layer with 100 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3151cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accs_MLP, coefs_MLP = run_cv(\n",
    "    shuffle=False,\n",
    "    data_key=data_key,\n",
    "    condmeans=condmeans,\n",
    "    clf=MLPClassifier(hidden_layer_sizes=(128), max_iter=500),\n",
    "    varthresh=None,\n",
    "    nfeatures=None,\n",
    ")\n",
    "results_df['MLP'] = [accs_MLP[i]['orig'] for i in accs_MLP.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.boxplot(data=results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c6485",
   "metadata": {},
   "source": [
    "### Searchlight decoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4612a6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
