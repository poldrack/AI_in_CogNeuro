{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for encoding/decoding models\n",
    "\n",
    "For this example, we will use the data from Haxby et al., 2001., which are shared via OpenNeuro:\n",
    "\n",
    "https://openneuro.org/datasets/ds000105/versions/3.0.0\n",
    "\n",
    "The data are formatted according to the BIDS standard: https://bids-specification.readthedocs.io/en/stable/index.html\n",
    "\n",
    "First, import required dependencies. You can install these using `pip install -r requirements.txt` from the main repo directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from nilearn import datasets, plotting\n",
    "from nilearn.image import load_img, mean_img, resample_img\n",
    "from nilearn.maskers import NiftiMasker\n",
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "from nilearn.plotting import plot_stat_map, plot_design_matrix\n",
    "import h5py\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import datalad.api as dl\n",
    "from bids import BIDSLayout\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from templateflow import api as tflow\n",
    "import templateflow\n",
    "from utils import (get_difumo_mask, \n",
    "                   get_subject_common_brain_mask,\n",
    "                   get_group_common_mask,\n",
    "                   get_subject_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the data using datalad\n",
    "\n",
    "We will use a tool called [Datalad](https://www.datalad.org/) to obtain the data from openneuro. \n",
    "\n",
    "We will download the raw data, as well as the processed data (using [fMRIPrep](https://fmriprep.org/en/stable/).  Note that downloading these derivative data can take quite a while depending on the speed of one's connection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Ensuring presence of Dataset(/Users/poldrack/data_unsynced/ds000105) to get /Users/poldrack/data_unsynced/ds000105 \n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/poldrack/data_unsynced/ds000105\"\n",
    "assert os.path.exists(data_dir), \"Data directory not found: %s\" % data_dir\n",
    "\n",
    "output_dir = os.path.join(data_dir, 'derivatives', 'glm')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "subdirs = ['zmaps', 'betas', 'subject_betas', 'group_zmaps']\n",
    "for d in subdirs:\n",
    "    if not os.path.exists(os.path.join(output_dir, d)):\n",
    "        os.makedirs(os.path.join(output_dir, d))\n",
    "\n",
    "# get the raw data\n",
    "ds = dl.clone(\n",
    "    path=data_dir,\n",
    "    source=\"https://github.com/OpenNeuroDatasets/ds000105.git\",\n",
    ")\n",
    "dl.get(dataset=data_dir, recursive=True)\n",
    "\n",
    "get_fmriprep = False  #set to false after downloading fmriprep once\n",
    "fmriprep_dir = os.path.join(data_dir, 'derivatives', 'fmriprep')\n",
    "\n",
    "# get the preprocessed derivatives - this takes some time!\n",
    "if get_fmriprep:\n",
    "    dl.clone(\n",
    "        path=fmriprep_dir,\n",
    "        source='https://github.com/OpenNeuroDerivatives/ds000105-fmriprep.git')\n",
    "    dl.get(dataset=fmriprep_dir, recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the dataset using PyBIDS\n",
    "\n",
    "Because the dataset is organized using the BIDS standard, we can use the [PyBIDS](https://bids-standard.github.io/pybids/) tool to query the dataset and obtain useful metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poldrack/micromamba/envs/aineuro/lib/python3.12/site-packages/bids/layout/layout.py:516: UserWarning: Derivative indexing was requested, but no valid datasets were found in the specified locations ([PosixPath('/Users/poldrack/data_unsynced/ds000105/derivatives/fmriprep/derivatives')]). Note that all BIDS-Derivatives datasets must meet all the requirements for BIDS-Raw datasets (a common problem is to fail to include a 'dataset_description.json' file in derivatives datasets).\n",
      "Example contents of 'dataset_description.json':\n",
      "{\"Name\": \"Example dataset\", \"BIDSVersion\": \"1.0.2\", \"GeneratedBy\": [{\"Name\": \"Example pipeline\"}]}\n",
      "  warnings.warn(\"Derivative indexing was requested, but no valid \"\n"
     ]
    }
   ],
   "source": [
    "# load the dataset using pybids and get runs for each subject\n",
    "\n",
    "def get_layouts(data_dir, fmriprep_dir):\n",
    "    \n",
    "    layout = BIDSLayout(data_dir)\n",
    "    deriv_layout = BIDSLayout(fmriprep_dir, derivatives=True, validate=False)\n",
    "    return layout, deriv_layout\n",
    "\n",
    "layout, deriv_layout = get_layouts(data_dir, fmriprep_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create common mask for each subject\n",
    "\n",
    "Each run will have slightly different voxels included in its brain mask, but we want to have a common mask across all runs, so we will generate a mask that includes the intersection of masks across all of the individual subs/runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_mask = get_group_common_mask(layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit linear models for each subject/run\n",
    "\n",
    "For each subject/run, we will load the preprocessed data and fit a model based on the task, along with confound regressors for head motion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_design_matrix(events, bold_img, deriv_layout, confounds=None,\n",
    "                      drift_model='Cosine', high_pass=0.01, hrf_model='spm + derivative'):\n",
    "    # define metadata for design matrix\n",
    "    n_scans = bold_img.shape[-1]\n",
    "    # Define the sampling times for the design matrix\n",
    "    t_r = deriv_layout.get_metadata(bold_file[0])['RepetitionTime']\n",
    "    frame_times = np.arange(n_scans) * t_r\n",
    "\n",
    "    design_matrix = make_first_level_design_matrix(\n",
    "        frame_times,\n",
    "        events,\n",
    "        hrf_model=hrf_model,\n",
    "        drift_model=drift_model,\n",
    "        high_pass=high_pass,\n",
    "        add_regs=confounds.values if confounds is not None else None,\n",
    "    )\n",
    "    return design_matrix, t_r\n",
    "\n",
    "\n",
    "def get_events(subject, run, layout):\n",
    "    # events file lives in the raw bold dir so we use the raw layout\n",
    "    events_file = layout.get(subject=subject, run=run, datatype='func', \n",
    "                             suffix='events', extension='tsv', return_type='file')\n",
    "    assert len(events_file) == 1, f\"Found {len(events_file)} events files for {subject} {run}\"\n",
    "    events = pd.read_csv(events_file[0], sep='\\t')\n",
    "    return events\n",
    "\n",
    "\n",
    "def get_bold_img(subject, run, deriv_layout):\n",
    "    bold_file = deriv_layout.get(datatype='func', desc='preproc', suffix='bold',\n",
    "                            space='MNI152NLin2009cAsym',\n",
    "                            subject=subject, run=f'{run:02d}',\n",
    "                            extension='nii.gz', return_type='file')\n",
    "    assert len(bold_file) == 1\n",
    "    bold_img = nib.load(bold_file[0])\n",
    "    return bold_file, bold_img\n",
    "\n",
    "\n",
    "def get_confounds(deriv_layout):\n",
    "    confound_files = deriv_layout.get(datatype='func', desc='confounds', suffix='timeseries',\n",
    "                                      subject=subject, run='01', extension='tsv', return_type='file')\n",
    "    assert len(confound_files) == 1\n",
    "    confounds = pd.read_csv(confound_files[0], sep='\\t')\n",
    "    confounds = confounds[[i for i in confounds.columns if 'rot' in i or 'trans' in i]]\n",
    "    confounds = confounds.bfill()  # replace NaNs\n",
    "    return confounds\n",
    "\n",
    "\n",
    "# test the functions\n",
    "subject = 1\n",
    "run = 1\n",
    "bold_file, bold_img = get_bold_img(subject, run, deriv_layout)\n",
    "events = get_events(subject, run, layout)\n",
    "confounds = get_confounds(deriv_layout)\n",
    "desmtx, t_r = get_design_matrix(events, bold_img, deriv_layout, confounds=confounds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate combined dataset\n",
    "\n",
    "Here we run the analysis on each subject/session, extract the data from a mask including the visual cortices (based on the [Difumo Atlas](https://parietal-inria.github.io/DiFuMo/)) and save them to an HDF5 file for later reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 12 runs for subject\n",
      "Processing subject 1 run 01\n",
      "Processing subject 1 run 02\n",
      "Processing subject 1 run 03\n",
      "Processing subject 1 run 04\n",
      "Processing subject 1 run 05\n",
      "Processing subject 1 run 06\n",
      "Processing subject 1 run 07\n",
      "Processing subject 1 run 08\n",
      "Processing subject 1 run 09\n",
      "Processing subject 1 run 10\n"
     ]
    }
   ],
   "source": [
    "def get_subject_data(subject, run, layout, deriv_layout, \n",
    "                     save_maps=True, plot_slices=False,\n",
    "                     hrf_model='spm + derivative'):\n",
    "    \"\"\"\n",
    "    Get the data for a single subject and run\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subject : str\n",
    "        subject ID\n",
    "    run : int\n",
    "        run number\n",
    "    layout : BIDSLayout\n",
    "        layout object for the raw data\n",
    "    deriv_layout : BIDSLayout\n",
    "        layout object for the preprocessed data\n",
    "    save_maps : bool\n",
    "        whether to save the zmaps and beta maps to disk\n",
    "    plot_slices : bool\n",
    "        whether to plot the slices of the zmaps\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data : numpy array\n",
    "        2D array of voxel data\n",
    "    conditions : list\n",
    "        list of condition names\n",
    "    \"\"\"\n",
    "    \n",
    "    run = int(run)\n",
    "\n",
    "    # load preprocessed BOLD data\n",
    "    bold_file, bold_img = get_bold_img(1, 1, deriv_layout)\n",
    "\n",
    "    # load brain mask\n",
    "    mask_img = get_group_common_mask(layout)\n",
    "    mask_data = mask_img.get_fdata().astype(bool)\n",
    "\n",
    "    events = get_events(subject, run, layout)\n",
    "\n",
    "    confounds = get_confounds(deriv_layout)\n",
    "\n",
    "    design_matrix, t_r = get_design_matrix(events, bold_img, deriv_layout, \n",
    "                                      confounds=confounds)\n",
    "    \n",
    "    # specify and fit the GLM to the bold data\n",
    "    fmri_glm = FirstLevelModel(t_r, noise_model='ar1', standardize=False, \n",
    "                               hrf_model=hrf_model)\n",
    "    fmri_glm = fmri_glm.fit(bold_img, design_matrices=design_matrix)\n",
    "\n",
    "    # compute contrasts\n",
    "    conditions = events.trial_type.unique()\n",
    "    conditions.sort()\n",
    "    z_map = {}\n",
    "    for condition in conditions:\n",
    "        contrast = fmri_glm.compute_contrast(condition, output_type='all')\n",
    "        z_map[condition] = contrast['z_score']\n",
    "        if save_maps:\n",
    "            contrast['z_score'].to_filename(os.path.join(output_dir,'zmaps', f'sub-{subject}_run-{run}_zmap_{condition}.nii.gz'))\n",
    "            contrast['effect_size'].to_filename(os.path.join(output_dir, 'betas', f'sub-{subject}_run-{run}_beta_{condition}.nii.gz'))\n",
    "\n",
    "    z_maps = nib.concat_images([z_map[condition] for condition in conditions])\n",
    "    \n",
    "    difumo_mask = get_difumo_mask()\n",
    "    difumo_mask = resample_img(difumo_mask, target_affine=z_maps.affine, \n",
    "                               target_shape=z_maps.shape[:3],\n",
    "                               interpolation='nearest')\n",
    "    masker = NiftiMasker(mask_img=difumo_mask, standardize=True, target_affine=z_maps.affine)\n",
    "    data = masker.fit_transform(z_maps)\n",
    "    assert data.shape == (len(conditions), difumo_mask.get_fdata().sum())\n",
    "    return data, conditions\n",
    "\n",
    "\n",
    "with h5py.File(os.path.join(output_dir, 'visctx_data.h5'), 'w') as hf:\n",
    "    for subject in layout.get_subjects():\n",
    "        sub_runs = get_subject_runs(subject, layout.root)\n",
    "        g1 = hf.create_group(f'sub-{subject}')\n",
    "        for run in sub_runs:\n",
    "            g2 = g1.create_group(f'run-{run}')\n",
    "            print(f\"Processing subject {subject} run {run}\")\n",
    "            data, conditions = get_subject_data(subject, run, layout, deriv_layout)\n",
    "            g2.create_dataset(f'voxdata',data=data)\n",
    "            g2.create_dataset(f'conditions', data=[c.encode('utf-8') for c in conditions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate group statistical maps\n",
    "\n",
    "As a reality check it is useful to compute a group statistical map for each of the conditions.  We first need to combine the data across runs within each subject, and then we do a group analysis.  This is a very underpowered analysis given the number of subjects, but it's useful for checking the modeling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1057322a0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/poldrack/micromamba/envs/aineuro/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 12 runs for subject\n",
      "found 12 runs for subject\n",
      "found 12 runs for subject\n"
     ]
    }
   ],
   "source": [
    "# combine across runs within each subject\n",
    "\n",
    "for subject in layout.get_subjects():\n",
    "    for condition in conditions:\n",
    "        subject_runs = get_subject_runs(subject, layout.root)\n",
    "        beta_files = [os.path.join(output_dir, 'betas', f'sub-{subject}_run-{int(run)}_beta_{condition}.nii.gz') for run in subject_runs]\n",
    "        model = SecondLevelModel()\n",
    "        model.fit(beta_files, design_matrix=pd.DataFrame([1] * len(beta_files), columns=['intercept']))\n",
    "        beta = model.compute_contrast(output_type='effect_size')\n",
    "        beta.to_filename(os.path.join(output_dir, 'subject_betas', f'sub-{subject}_FEbeta_{condition}.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run group model\n",
    "\n",
    "for condition in conditions:\n",
    "    beta_files = [os.path.join(output_dir, 'subject_betas', f'sub-{subject}_FEbeta_{condition}.nii.gz') for subject in layout.get_subjects()]\n",
    "    model = SecondLevelModel()\n",
    "    model.fit(beta_files, design_matrix=pd.DataFrame([1] * len(beta_files), columns=['intercept']))\n",
    "    z_map = model.compute_contrast(output_type='z_score')\n",
    "    z_map.to_filename(os.path.join(output_dir, 'group_zmaps', f'group_zmap_{condition}.nii.gz'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the maps for a quick reality check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition in conditions:\n",
    "    z_map = load_img(os.path.join(output_dir, 'group_zmaps', f'group_zmap_{condition}.nii.gz'))\n",
    "    plotting.plot_stat_map(z_map, title=condition, threshold=2.0, display_mode='z', cut_coords=6)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
